@BOOK{Lamport86,
TITLE = "{LaTeX} --- a document preparation system --- user's guide
and reference manual",
AUTHOR = "Lamport, L.",
PUBLISHER = "Addison-Wesley",
YEAR = "1986"}

@ARTICLE{parallelPerformance,
AUTHOR = "Karp and Alan and Flatt, Horace",
YEAR = {1990},
MONTH = {05},
PAGES = {539-543},
TITLE = {Measuring Parallel Processor Performance},
VOLUME = {33},
JOURNAL = {Communications of the ACM},
DOI = {10.1145/78607.78614}
}

@article{fox,
title = {Matrix algorithms on a hypercube I: Matrix multiplication},
journal = {Parallel Computing},
volume = {4},
number = {1},
pages = {17-31},
year = {1987},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(87)90060-3},
url = {https://www.sciencedirect.com/science/article/pii/0167819187900603},
author = {G.C Fox and S.W Otto and A.J.G Hey},
keywords = {Matrix multiplication, Caltech Hypercube, performance analysis, communication overhead, load balancing},
abstract = {We discuss algorithms for matrix multiplication on a concurrent processor containing a two-dimensional mesh or richer topology. We present detailed performance measurements on hypercubes with 4, 16, and 64 nodes, and analyze them in terms of communication overhead and load balancing. We show that the decomposition into square subblocks is optimal C code implementing the algorithms is available.}
}

@inproceedings{graph-dataset,
    author = {Li, Feifei and Cheng, Dihan and Hadjieleftheriou, Marios and Kollios, George and Teng, Shang-Hua},
    year = {2005},
    month = {08},
    pages = {273-290},
    title = {On Trip Planning Queries in Spatial Databases},
    volume = {3633},
    isbn = {978-3-540-28127-6},
    journal = {Lecture Notes in Computer Science},
    doi = {10.1007/11535331_16}
}

@misc{sandyBridge,
    author = {{Sandy Bridge (client) - Microarchitectures - Intel}},
    title = "Sandy Bridge (client) - Microarchitectures - Intel --- {W}iki{C}hip",
    year = "2020",
    url = "https://en.wikichip.org/w/index.php?title=intel/microarchitectures/sandy_bridge_(client)\&oldid=98305",
    note = "[Online; accessed 24-April-2022]"
}

@article{timJones,
    author = {Campanoni, Simone and Brownell, Kevin and Kanev, Svilen and Jones, Timothy M. and Wei, Gu-Yeon and Brooks, David},
    title = {Automatically Accelerating Non-Numerical Programs by Architecture-Compiler Co-Design},
    year = {2017},
    issue_date = {December 2017},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {60},
    number = {12},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3139461},
    doi = {10.1145/3139461},
    journal = {Commun. ACM},
    month = {nov},
    pages = {88–97},
    numpages = {10}
}

@techreport{sunway,
    title = {Report on the Sunway TaihuLight System},
    journal = {University of Tennessee Computer Science Technical Report},
    number = {UT-EECS-16-742},
    year = {2016},
    month = {2016-06},
    publisher = {University of Tennessee},
    url = {http://www.netlib.org/utk/people/JackDongarra/PAPERS/sunway-report-2016.pdf},
    author = {Jack Dongarra}
}

@misc{broadband,
    title={Worldwide broadband speed league 2021},
    url={https://www.cable.co.uk/broadband/speed/worldwide-speed-league/},
    journal={cable.co.uk},
    author={Howdle, Dan},
    year = {2021},
    note = "[Online; accessed 24-April-2022]"
}

@misc{verizon,
    title = {{IP} Latency Statistics},
    url = {https://www.verizon.com/business/terms/latency/},
    year = {2022},
    journal = {verizon.com},
    author = {Verizon},
    note = "[Online; accessed 24-April-2022]"
}

@misc{hockney,
    title = {{Introduction to Parallel Programming}, {S}ection 8.
             Parallel Methods for Matrix Multiplication},
    author = {Victor P Gergel},
    howpublished = "\url{http://www.hpcc.unn.ru/mskurs/ENG/DOC/pp08.pdf}",
    year= {2005},
    note = "[Online; accessed 24-April-2022]"
}

@article{sparseMatMul,
    author    = {Aydin Bulu{\c{c}} and
                 John R. Gilbert},
    title     = {Highly Parallel Sparse Matrix-Matrix Multiplication},
    journal   = {CoRR},
    volume    = {abs/1006.2183},
    year      = {2010},
    url       = {http://arxiv.org/abs/1006.2183},
    eprinttype = {arXiv},
    eprint    = {1006.2183},
    timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-1006-2183.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{parallelDijkstra,
    author = {Crauser, Andreas and Mehlhorn, Kurt and Meyer, Ulrich and Sanders, Peter and Brim, Luboš and Gruska, Jozef and Zlatuska, Jiri},
    year = {1998},
    month = {10},
    pages = {},
    title = {A Parallelization of Dijkstra's Shortest Path Algorithm},
    isbn = {978-3-540-64827-7},
    journal = {Proceedings of the 23rd International Symposium on the Mathematical Foundations of Computer Science (MFCS-98), Springer, 722-731 (1998)},
    doi = {10.1007/BFb0055823}
}

@article{berntsen,
    title = {Communication efficient matrix multiplication on hypercubes},
    journal = {Parallel Computing},
    volume = {12},
    number = {3},
    pages = {335-342},
    year = {1989},
    issn = {0167-8191},
    doi = {https://doi.org/10.1016/0167-8191(89)90091-4},
    url = {https://www.sciencedirect.com/science/article/pii/0167819189900914},
    author = {Jarle Berntsen},
    keywords = {Matrix multiplication, hypercubes, communication overhead, block algorithms},
}

@article{astar,
  author    = {Alex Fukunaga and
               Adi Botea and
               Yuu Jinnai and
               Akihiro Kishimoto},
  title     = {A Survey of Parallel {A}},
  journal   = {CoRR},
  volume    = {abs/1708.05296},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.05296},
  eprinttype = {arXiv},
  eprint    = {1708.05296},
  timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-05296.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{floydScale,
    title = {Scalable All-pairs Shortest Paths for Huge Graphs on Multi-GPU Clusters},
    author = {Sao, Piyush and Lu, Hao and Kannan, Ramakrishnan and Thakkar, Vijay and Vuduc, Richard and Potok, Thomas},
    doi = {},
    url = {https://www.osti.gov/biblio/1814306}, journal = {},
    place = {United States},
    year = {2020},
    month = {06}
} 

@article{scalability,
    title = {Scalability of parallel algorithms for the all-pairs shortest-path problem},
    journal = {Journal of Parallel and Distributed Computing},
    volume = {13},
    number = {2},
    pages = {124-138},
    year = {1991},
    issn = {0743-7315},
    doi = {https://doi.org/10.1016/0743-7315(91)90083-L},
    url = {https://www.sciencedirect.com/science/article/pii/074373159190083L},
    author = {Vipin Kumar and Vineet Singh},
}

@inproceedings{experimentalAnalysis,
    author = {Kim, Jong Wook and Choi, Hyoeun and Bae, Seung-Hee},
    title = {Efficient Parallel All-Pairs Shortest Paths Algorithm for Complex Graph Analysis},
    year = {2018},
    isbn = {9781450365239},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3229710.3229730},
    doi = {10.1145/3229710.3229730},
    abstract = {The all-pairs shortest path problem is a classic problem to study characteristics of the given graphs. Though many efficient all-pairs shortest path algorithms have been published, it is still a very expensive computing task, especially with large graph datasets. In this paper, we propose an efficient parallel all-pairs shortest path algorithm based on Peng et al.'s fast sequential algorithm on shared-memory parallel environments to achieve faster and more efficient calculation for large-scale real-world networks. Peng et al.'s algorithm needs to sort vertices with respect to their degrees. However, it turns out the original algorithm uses less efficient sorting method, which is a significant portion of parallel overhead. Therefore, we also propose an efficient parallel method to sort data within a fixed range, in order to minimize the parallel overhead in our parallel algorithm. The optimized efficient sorting method can be used for general sorting purposes. Our experimental analysis shows that our proposed parallel algorithm achieves very high parallel speedup, even hyper-linear speedup, with real-world test datasets on two different shared-memory multi-core systems.},
    booktitle = {Proceedings of the 47th International Conference on Parallel Processing Companion},
    articleno = {5},
    numpages = {10},
    keywords = {Shared-memory parallelism, Parallel Algorithms, All-pairs shortest paths},
    location = {Eugene, OR, USA},
    series = {ICPP '18}
}
